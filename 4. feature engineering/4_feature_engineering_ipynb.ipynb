{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ERsMF476hFv",
        "outputId": "d0978f13-6b5a-4c89-b001-6f5cd9ce8d41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import  RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import pickle\n",
        "from google.cloud import bigquery\n",
        "import math\n",
        "import re\n",
        "#!pip install xgboost\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import ndcg_score\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "num_folds = 4\n",
        "from nltk import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns\n",
        "#!pip install gensim\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "from nltk.stem import PorterStemmer \n",
        "pd.options.mode.chained_assignment = None\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def common_words(df, col1, col2):\n",
        "      \"\"\"\n",
        "      Returns common words between each row of col1 and col2 of df in the form of a list. \n",
        "      Length of list is number of rows in dataframe\n",
        "      \"\"\"\n",
        "      common_list = []\n",
        "      for i, row in df[[col1,col2]].iterrows():\n",
        "        set1 = set(str(row[col1]).split())\n",
        "        set2 = set(str(row[col2]).split())\n",
        "        common = set1 & set2\n",
        "        common = ' '.join(common)\n",
        "        common_list.append(common)\n",
        "      return common_list\n",
        "\n",
        "def cosine_similarity_sent(sent1, sent2):\n",
        "      \"\"\"\n",
        "      Cosine Similarity between 2 sentences treating them as sets of words\n",
        "      \"\"\"\n",
        "\n",
        "      set1 = set(str(sent1).split())\n",
        "      set2 = set(str(sent2).split())\n",
        "      numerator = len(set1 & set2)\n",
        "      denominator = math.sqrt(len(set1)) * math.sqrt(len(set2))\n",
        "\n",
        "      if not denominator:\n",
        "          return 0.0\n",
        "      else:\n",
        "          return numerator / denominator\n",
        "    \n",
        "def jacquard_coefficient_sent(sent1, sent2):\n",
        "      \"\"\"\n",
        "      Jacquard Coefficient between 2 sentences treating them as sets of words\n",
        "      \"\"\"\n",
        "\n",
        "      set1 = set(str(sent1).split())\n",
        "      set2 = set(str(sent2).split())\n",
        "      numerator = len(set1 & set2)\n",
        "      denominator = len(set1 | set2)\n",
        "\n",
        "      if not denominator:\n",
        "          return 0.0\n",
        "      else:\n",
        "          return numerator / denominator\n",
        "\n",
        "#Calculating the distance columns\n",
        "\n",
        "#Common words between first part of query and all other product hierarch actual columns\n",
        "def common_word_for_colums(df,x,y):\n",
        "    \n",
        "    for col in y:\n",
        "        df = pd.concat([df,pd.Series(common_words(df,x,col))],axis=1).rename({0:str(col)+\"_q_common_words\"},axis=1)\n",
        "        df[str(col)+\"_\"+str(x)+\"_common_words\"] = 0\n",
        "        df[str(col)+\"_\"+str(x)+\"_common_words\"] = df[str(col)+\"_\"+str(x)+\"_common_words\"].apply(lambda x : len(str(x).split()))\n",
        "    return df\n",
        "\n",
        "def cosine_similarity_for_columns(df,col1,col2):\n",
        "    for col in col2:\n",
        "        df[str(col)+\"_\"+str(col1)+\"_cosine_similarity\"] = 0\n",
        "        df[str(col)+\"_\"+str(col1)+\"_cosine_similarity\"] = df.apply(lambda x : cosine_similarity_sent(x[col1],x[col]),axis=1) \n",
        "    return df\n",
        "\n",
        "def to_lower_cols(df,cols):\n",
        "    for col in cols:\n",
        "        df[col] = df[col].apply(lambda x: x.lower())\n",
        "    return df\n",
        "\n",
        "def remove_spl_chars(df,cols):\n",
        "    for col in cols:\n",
        "        df[col] = df[col].apply(lambda x: re.sub('[^a-zA-Z0-9 \\n\\.]', ' ', x))\n",
        "    return df\n",
        "\n",
        "\n",
        "def query_product_similarity_based_features(df):\n",
        "    \n",
        "    '''\n",
        "    Objective:\n",
        "    * Processing the features to calucate similarity based features between query and product description\n",
        "    * Calcualting the similary between product description and query\n",
        "    \n",
        "    '''\n",
        "\n",
        "    \n",
        "    #Extracting features from queries and product description\n",
        "    df['first_part_query'] = df['imp_path'].apply(lambda x: x.split(\"/\")[1].lower())\n",
        "    df['second_part_query'] = df['imp_path'].apply(lambda x: x.split(\"/\")[2].lower())\n",
        "\n",
        "    df =  remove_spl_chars(df,cols=['first_part_query','second_part_query'])\n",
        "\n",
        "    df['gender_query'] = np.where(df['gender_clean']=='Female','women','men')\n",
        "\n",
        "    df = to_lower_cols(df,cols=['gender_query','department','category_clean','subcategory_clean','brand_clean','occasion','basic_type','product_detail','merch_team_clean','color'])\n",
        "\n",
        "    df['new_arrival'] = np.where(df['time_since_launch']<=30, \"new arrivals\",\"old\")\n",
        "\n",
        "    df['prod_hierarychy_actual'] = df['gender_query'] + \" \" + df['department'] + \" \" + df['category_clean'] + \" \" + df['subcategory_clean']\n",
        "    \n",
        "    df = common_word_for_colums(df = df,x='first_part_query', y=['prod_hierarychy_actual','gender_query','department','category_clean','subcategory_clean','brand_clean','occasion','basic_type','product_detail','merch_team_clean','color'])    \n",
        "    df = cosine_similarity_for_columns(df=df,col1='first_part_query',col2 = ['prod_hierarychy_actual','gender_query','department','category_clean','subcategory_clean','brand_clean','occasion','basic_type','product_detail','merch_team_clean','color'])\n",
        "    df = common_word_for_colums(df = df,x='second_part_query', y=['prod_hierarychy_actual','gender_query','department','category_clean','subcategory_clean','brand_clean','occasion','basic_type','product_detail','merch_team_clean','color'])    \n",
        "    df = cosine_similarity_for_columns(df=df,col1='second_part_query',col2 = ['prod_hierarychy_actual','gender_query','department','category_clean','subcategory_clean','brand_clean','occasion','basic_type','product_detail','merch_team_clean','color']) \n",
        "    \n",
        "    return df\n",
        "\n",
        "def calc_ctr(df,level,name):\n",
        "    '''\n",
        "    Objective: \n",
        "    Calcualting the click through rate, total impressions, total clicks at given level\n",
        "    \n",
        "    '''\n",
        "    temp=df.groupby(level,as_index=False)['total_impressions_lp','total_clicks_lp'].sum()\n",
        "    temp[['total_impressions_lp','total_clicks_lp']] = temp[['total_impressions_lp','total_clicks_lp']].astype(int)\n",
        "    temp[str(name)+'_ctr'] = np.round(np.where(temp['total_impressions_lp']==0,0,temp['total_clicks_lp']/temp['total_impressions_lp'])*100,2)\n",
        "    temp.rename({'total_impressions_lp': str(name)+'_impression_lp','total_clicks_lp': str(name)+'clicks_lp' },axis=1,inplace=True)\n",
        "    #temp.drop(['total_impressions_lp','total_clicks_lp'],axis=1,inplace=True)\n",
        "    df = df.merge(temp,on=level,how='left')\n",
        "    return df\n",
        "\n",
        "def historical_performance_features(df_sql3, df):\n",
        "    \n",
        "    '''\n",
        "    Objective:\n",
        "    To create historical performance based features at different levels\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    #Calculating total impressions and click at query and product level\n",
        "    temp = df_sql3.groupby(['imp_path','sku'],as_index=False)['total_impressions','total_clicks'].sum().rename({'total_impressions':'total_impressions_lp','total_clicks':'total_clicks_lp'},axis=1)\n",
        "    \n",
        "    #Aggregation imp_position level\n",
        "    temp2 = df_sql3.groupby(['imp_path','sku','imp_position'],as_index=False)['total_impressions','total_clicks'].sum().rename({'total_impressions':'total_impressions_lp','total_clicks':'total_clicks_lp'},axis=1)\n",
        "    \n",
        "    temp2['imp_bucket'] = np.select([temp2['imp_position'] <=10,\n",
        "                                     (temp2['imp_position'] >10) & (temp2['imp_position'] <=20),\n",
        "                                     (temp2['imp_position'] >20) & (temp2['imp_position'] <=30),\n",
        "                                     (temp2['imp_position'] >30) & (temp2['imp_position'] <=40),\n",
        "                                     (temp2['imp_position'] >40) & (temp2['imp_position'] <=50)\n",
        "                                    ],\n",
        "                                   ['imp_pos_0_10',\n",
        "                                    'imp_pos_11_20',\n",
        "                                    'imp_pos_21_30',\n",
        "                                    'imp_pos_31_40',\n",
        "                                    'imp_pos_41_50'                                       \n",
        "                                   ], 'ndefined'\n",
        "                                   )\n",
        "    \n",
        "    temp2 = temp2.groupby(['imp_path','sku','imp_bucket'],as_index=False)['total_clicks_lp','total_impressions_lp'].sum()\n",
        "    \n",
        "    \n",
        "    temp3= temp2.pivot(index=['imp_path','sku'],values='total_clicks_lp',columns='imp_bucket')\n",
        "    temp3.rename({'imp_pos_0_10':'imp_pos_0_10_clicks','imp_pos_11_20':'imp_pos_11_20_clicks',\n",
        "                 'imp_pos_21_30':'imp_pos_21_30_clicks','imp_pos_31_40':'imp_pos_31_40_clicks','imp_pos_41_50':'imp_pos_41_50_clicks'},axis=1,inplace=True)\n",
        "    \n",
        "    temp4 = temp2.pivot(index=['imp_path','sku'],values='total_impressions_lp',columns='imp_bucket')\n",
        "    temp3.rename({'imp_pos_0_10':'imp_pos_0_10_imps','imp_pos_11_20':'imp_pos_11_20_imps',\n",
        "                 'imp_pos_21_30':'imp_pos_21_30_imps','imp_pos_31_40':'imp_pos_31_40_imps','imp_pos_41_50':'imp_pos_41_50_imps'},axis=1,inplace=True)\n",
        "    \n",
        "    df2 = df.merge(temp,on=['imp_path','sku'],how='left')\n",
        "    df2 = df2.merge(temp3,on=['imp_path','sku'],how='left')\n",
        "    df2 = df2.merge(temp4,on=['imp_path','sku'],how='left')\n",
        "\n",
        "    #levels at which to calculate the CTR usning above function\n",
        "    levels = [ #CTR variables at 3 levels\n",
        "                 ['imp_path', 'brand_clean', 'department'],\n",
        "                 ['imp_path', 'brand_clean', 'category_clean'],\n",
        "                 ['imp_path', 'brand_clean', 'subcategory_clean'],\n",
        "                 ['imp_path', 'brand_clean', 'gender_clean'],\n",
        "                 ['imp_path', 'department', 'category_clean'],\n",
        "                 ['imp_path', 'department', 'subcategory_clean'],\n",
        "                 ['imp_path', 'department', 'gender_clean'],\n",
        "                 ['imp_path', 'category_clean', 'subcategory_clean'],\n",
        "                 ['imp_path', 'category_clean', 'gender_clean'],\n",
        "                 ['imp_path', 'subcategory_clean', 'gender_clean'],\n",
        "                 ['brand_clean', 'department', 'category_clean'],\n",
        "                 ['brand_clean', 'department', 'subcategory_clean'],\n",
        "                 ['department', 'category_clean', 'subcategory_clean'],\n",
        "\n",
        "                #CTR variables at 2 levels\n",
        "                  ['imp_path','brand_clean'],['imp_path','department'],\n",
        "                  ['imp_path','category_clean'],['imp_path','subcategory_clean'],\n",
        "                  ['imp_path','gender_clean'],['imp_path','color'],\n",
        "                  ['imp_path','occasion'],['imp_path','inventory_group_clean'],\n",
        "                  ['imp_path','basic_type'],['imp_path','specialist'],\n",
        "                  ['imp_path','merch_team_clean'],['imp_path','time_since_launch_bucket'],\n",
        "                  ['imp_path','time_since_relaunch_bucket'],['imp_path','discount_bucket'],\n",
        "                  ['brand_clean', 'department'],['brand_clean', 'category_clean'],\n",
        "                  ['brand_clean', 'subcategory_clean'],['brand_clean', 'gender_clean'],\n",
        "                  ['department', 'category_clean'],['department', 'subcategory_clean'],\n",
        "                  ['department', 'gender_clean'],['category_clean', 'subcategory_clean'],\n",
        "                  ['category_clean', 'gender_clean'],['subcategory_clean', 'gender_clean'],\n",
        "                  ['time_since_launch_bucket','brand_clean'],['time_since_launch_bucket','category_clean'],\n",
        "                  ['time_since_launch_bucket','subcategory_clean'],['time_since_launch_bucket','gender_clean'],\n",
        "                  ['discount_bucket','brand_clean'],['discount_bucket','category_clean'],\n",
        "                  ['discount_bucket','subcategory_clean'],['discount_bucket','gender_clean'],\n",
        "\n",
        "              #CTR variables at 1 level\n",
        "                 ['brand_clean'],['department'],['category_clean'],['subcategory_clean'],\n",
        "                 ['gender_clean'],['color'],['occasion'],['inventory_group_clean'],\n",
        "                 ['basic_type'],['specialist'],['merch_team_clean'],['time_since_launch_bucket'],\n",
        "                 ['time_since_relaunch_bucket'],['discount_bucket']]\n",
        "\n",
        "\n",
        "    names = ['imp_brand_dept','imp_brand_cat', 'imp_brand_subcat', 'imp_brand_gender','imp_dept_cat','imp_dept_subcat','imp_dept_gender','imp_cat_subcat','imp_cat_gender','imp_subcat_gender',\n",
        "             'brand_dept_cat','brand_dept_subcat','brand_cat_subcat',\n",
        "\n",
        "             'imp_brand','imp_dept','imp_cat','imp_subcat','imp_gender','imp_color','imp_occasion','imp_inv_group','imp_basic_type','imp_specialist',\n",
        "            'imp_merch_team_clean','imp_time_since_launch_bucket','imp_time_since_relaunch_bucket','imp_discount_bucket','brand_dept','brand_cat','brand_subcat','brand_gender',\n",
        "             'dept_cat','dept_subcat','dept_gender','cat_subcat','cat_gender','subcat_gender','time_launch_brand','time_launch_cat','time_launch_subcat','time_launch_gender',\n",
        "             'disc_bucket_brand','disc_bucket_cat','disc_bucket_subcat','disc_bucket_gender',\n",
        "\n",
        "             'brand','dept','cat','subcat','gender','color','occasion','inv_group','basic_type','specialist','merch_team_clean','time_since_launch_bucket',\n",
        "             'time_since_relaunch_bucket','discount_bucket']\n",
        "\n",
        "    for cols,name in zip(levels,names):\n",
        "        df2 = calc_ctr(df2,cols,name)\n",
        "    df2[['total_impressions_lp','total_clicks_lp']] = df2[['total_impressions_lp','total_clicks_lp']].fillna(0)\n",
        "    df2[['total_impressions_lp','total_clicks_lp']] = df2[['total_impressions_lp','total_clicks_lp']].astype(int)\n",
        "    df2['ctr_lp'] = np.round(np.where(df2['total_impressions_lp']==0,0,df2['total_clicks_lp']/df2['total_impressions_lp'])*100,2)\n",
        "        \n",
        "    return df2"
      ],
      "metadata": {
        "id": "w46MsrUP7EJm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Inputs\n",
        "df_query3 =  pd.read_pickle('drive/MyDrive/Training Data/stage 1/df_query3.pkl')\n",
        "df = pd.read_csv('drive/MyDrive/Training Data/stage 3/relevance_feature_added_ts.csv')\n",
        "\n",
        "#Adding query similarity based features\n",
        "df2 = query_product_similarity_based_features(df)\n",
        "\n",
        "df3 = historical_performance_features(df_query3, df2)\n",
        "\n",
        "#Outputs\n",
        "df3.to_csv('drive/MyDrive/Training Data/stage 4/final_training_dataset.csv')"
      ],
      "metadata": {
        "id": "N3edqds77PFg"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}